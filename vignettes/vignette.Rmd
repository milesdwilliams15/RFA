---
title: "Random Forest Adjustment with the `RFA` package"
author: "Miles D. Williams (University of Illinois at Urbana-Champaign, milesdw2@illinois.edu)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

### Details

The repository for this project can be accessed [here](https://github.com/milesdwilliams15/RFA). Check out [my website](https://milesdwilliams15.github.io/) to learn more about me and my research.

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(tidyverse)
results <- read_csv(paste0(getwd(),"/results.csv"))
```

# Introduction

Random Forest Adjustment (RFA) is a procedure for estimating the relationship between some predictor and a response that takes a flexible approach to adjusting for the confounding influence of covariates. As the name implies, the routine uses random forest regression to adjust for covariates. Below I provide a brief explanation of the technique and demonstrate how it is implemented in R.

# RFA in a Nutshell

RFA provides an alternative to multiple regression adjustment for controlling for covariates in observational settings. By leveraging a nonparametric machine learning algorithm such as random forests, RFA sidesteps the incidental functional form assumptions imposed by regression based adjustment---additivity and linearity in confounding relationships---thus avoiding potential bias in the recovered marginal relationship between an explanatory variable and response incurred by model mispecification. 

RFA begins by predicting some response $y_i$ and some causal variable of interest $z_i$ as a function of covariates $x_{ip} \in X_i$ where $y_i,z_i \not\!\perp\!\!\!\perp X_i$. The variables $y_i$ and $z_i$ denote vectors of response and predictor values for the $i^\text{th}$ observation where $i \in I = \{1,2,...,n \}$. 

Because $z_i$ and $y_i$ are not independent of $X_i$, the estimated slope coefficient ($\alpha_1$) from the following naive OLS model will not reflect the correct relationship between $z_i$ on $y_i$:
$$y_i = \alpha_0 + \alpha_1z_i + \epsilon_i. \tag{1}$$
RFA's solution to this problem is to partial out the variation in $y_i$ and $z_i$ explained by $X_i$ prior to estimating this relationship using random forests to pre-process the explanatory variable and response.

  1. The first step is to fit $y_i$ as a function of $X_i$ via random forests (the random forest fit for $y_i$ is denoted $\hat{f}_y(X_i)$), and then demean, or residualize, $y_i$ by its predicted conditional mean:

$$
\begin{aligned}
\hat{y}_i & = \hat{f}_y(X_i), \\
\hat{y}_i^\varepsilon & = y_i - \hat{y}_i.
\end{aligned} \tag{2}
$$

  2. This step is then repeated for $z_i$:

$$
\begin{aligned}
\hat{z}_i & = \hat{f}_z(X_i), \\
\hat{z}_i^\varepsilon & = z_i - \hat{z}_i.
\end{aligned} \tag{3}
$$

  3. Finally, the marginal relationship, adjusting for the confounding influence of $X_i$, is obtained by estimating the following linear model:

$$\hat{y}_i^\varepsilon = \beta_0 + \beta_1\hat{z}_i^\varepsilon + \mu_i.\tag{4}$$


# What is the Benefit of This Approach?

The advantage of RFA is that it sidesteps incidental functional form assumptions that the more conventional multiple regression adjustment strategy imposes. This can be seen by considering the following fact about least squares regression, the workhorse adjustment strategy.

With the covariates given in the preceding section, define the matrix $\mathbf{W}$ as
$$
\mathbf{W} = 
\begin{bmatrix}
1 & z_1 & x_1^1 & \cdots & x_1^k \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & z_n & x_n^1 & \cdots & x_n^k
\end{bmatrix}_{n,k+2}
$$
and define the matrix $\mathbf{y}$ as
$$
\mathbf{y} = 
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix}_{n,1}.
$$
Using ordinary least squares (OLS), we estimate the linear model
$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon},$$
where the solution for the parameters to be estimated is
$$
\hat{\boldsymbol{\beta}} = 
\begin{bmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1 \\
\hat{\beta}_2^1 \\
\vdots \\
\hat{\beta}_{k+2}^k
\end{bmatrix} = (\mathbf{W}^\top\mathbf{W})^{-1} (\mathbf{W}^\top \mathbf{y}).
$$
The parameter $\hat{\beta}_1 \in \hat{\boldsymbol{\beta}}$ is the estimated effect of $z_i$ (the causal variable of interest in our running example).

This approach is potentially problematic for the following reason. Consider an equivalent way of generating $\hat{\beta}_1$ via OLS. 

  1. Estimate an OLS model excluding $z_i$ from the matrix $\mathbf{W}$ and with $\mathbf{y}$ as the left-hand side variable. Denote the new $n \times k + 1$ matrix that excludes the causal variable interest as $\mathbf{X}$. After estimating this model using the same procedure outlined above, generate predicted values of the outcome variable and estimate the model residuals (note the similarity to step 1 of RFA).
  
$$
\begin{aligned}
\hat{\mathbf{y}} & = \mathbf{X}\hat{\boldsymbol{\gamma}}, \\
\hat{\mathbf{y}}^\epsilon & = \mathbf{y} -  \hat{\mathbf{y}}.
\end{aligned}
$$

  2. Repeat the above, but have the $n \times 1$ matrix $z$ (a matrix of values for the causal variable) be the left-hand side variable (again, note the similarity to step 2 of RFA).
  
$$
\begin{aligned}
\hat{\mathbf{z}} & = \mathbf{X}\hat{\boldsymbol{\delta}}, \\
\hat{\mathbf{z}}^\epsilon & = \mathbf{z} -  \hat{\mathbf{z}}.
\end{aligned}
$$

  3. Finally, estimate $\beta_1$ via an OLS model specified as

$$\hat{y}_i^\epsilon = \mu + \beta_1\hat{z}^\epsilon_i + \varepsilon_i.$$

The $\hat{\beta}_1$ generated from step 3 will be equivalent to the one estimated above.

At this point, it should be clear why the parametric multiple regression framework is potentially problematic. Such an approach, while imposing the assumption that $y_i$ and $z_i$ are linearly related, also imposes the assumption that the confounding variables are linearly and additively associated with $y_i$ and $z_i$. And this equivalent way of expressing OLS given above is only the first layer of an onion of linear additive models within linear additive models which could be used to estimate equivalent parameters for all of the variables contained in $\mathbf{W}$. This is not to mention the googleplex of potential "sub"-parameters implied by assuming linearity and additivity *all the way down*. If in any of the linear additive models upon linear additive models is misspecified, this portends bias in $\hat{\beta}_1$.

This is where the power of RFA lies. RFA imposes no functional form on the relationship between the confounders and the response and causal variable, and it also imposes no functional form assumptions about how any and all of the confounders relate to each other. It sidesteps the Russian doll altogether as it isolates the residual variation in the response and causal variable.


# A Simulation

To hammer home the point that RFA is advantageous, consider the following results from a simulation I ran. Here are some of the details of said simulation:

  1. I set $n = 500$ and set the ATE ($\beta_1$) to 5.
  2. The "true" d.g.p. for the outcome variable $y_i$ is given as
  
$$y_i = 1 + 5z_i + 0.5x_i + x_i^2 + e_i:e_i \sim \mathcal{N}(0, \sigma = 10).$$

  3. The causal variable $z_i$ is given as
  
$$z_i = -1 + 0.05\space \text{stand}(x_i) - 0.1 \space \text{stand}(x_i)^2 + u_i: u_i \sim \mathcal{N}(0, \sigma = 2).$$

  4. The confounding variable $x_i \sim \mathcal{N}(50, \sigma = 10)$.

I try a number of methods to recover estimates of the ATE of $z_i$:

  1. A naive OLS model:
  
$$y_i = \beta_0 + \beta_1z_i + \epsilon_i.$$

  2. An OLS model with controls:
  
$$y_i = \beta_0 + \beta_1z_i + \beta_2x_i + \epsilon_i.$$

  3. An interaction model with $x_i$ mean centered:

$$y_i = \beta_0 + \beta_1z_i + \beta_2x_i + \beta_3z_i\cdot(x_i - \bar{x}) + \epsilon_i.$$

  4. Adjust via a Support Vector Machine (SVM)---I use the same steps as with RFA, save that I control for the confounding effect of $x_i$ with SVM.
  
  5. Finally, I use RFA.
  
Because both SVM and RF take a while to run, for time's sake I restricted my simulation to 100 iterations. 

A summary of the performance of each of these approaches is given in the below table. Metrics I use to gauge performance include root mean squared error (MSE), average bias, and coverage of the 95 percent confidence intervals. 

```{r, echo=FALSE}
results %>%
  group_by(model) %>%
  summarize(
    RMSE = sqrt(sum((estimate - ATE)^2)),
    "Ave. Bias" = mean(estimate - ATE),
    Coverage = mean((((estimate - 1.96*std.error)<=ATE) + 
                          ((estimate + 1.96*std.error)>=ATE))==2) 
  ) %>%
  mutate_if(is.numeric, function(x) round(x, 2)) %>%
  kableExtra::kable(format = "html")
```


RFA clearly performs the best out of all of the adjustment strategies. This is also clear from the following figure showing the distribution of estimated ATE replicates after 100 runs of the simulation. The estimates provided by RFA are the least prone to error and least biased. However, in terms of coverage, naive OLS, RFA, and SVA are relatively similar.

```{r, echo=FALSE, fig.width=6, fig.height=4}
ATE <- results$ATE[1]
results %>%
  select(-ATE) %>%
  dotwhisker::small_multiple(
    dot_args = list(alpha = 0.15)
  ) + 
  geom_hline(yintercept = c(0, ATE), linetype = c(1,2)) +
  labs(
    caption = "SVA = Support Vector Adjustment;\nRFA = Random Forest Adjustment\n(Dashed line denotes true effect)",
    y = "Estimate with 95% CI",
    title = "Comparison of Different Adjustment Strategies"
  ) +
  ggridges::theme_ridges(
    font_size = 10
  ) +
  theme(
    strip.background = element_blank(),
    strip.text = element_blank(),
    legend.position = "none"
  )
```


# How to Implement in R

The `rfa` function in the `RFA` package allows the researcher to easily implement the RFA routine. Here, I demonstrate how the program is used with the assistance of the `GerberGreenImai` dataset included in the `Matching` package. This dataset was used in Imai (2005) to replicate and extend Gerber and Green's (2000) get-out-the-vote (GOT) field experiment. The dataset was used to assess the causal effect of telephone calls on turnout. Note that this data is used for demonstration purposes only, and so results here should not be taken as confirmation or refutation of the findings of the original authors.

Let's get the data into the working environment:

```{r}
library(tidyverse) # grammar
library(Matching)  # for the data
data(GerberGreenImai)
ggi <- GerberGreenImai # give it a shorter name
```

The first thing to do is attach the `RFA` package.

```{r}
library(RFA)
```

If not already installed, the latest development version can be installed by entering `devtools::install_github("milesdwilliams15/RFA")` in the console.

Next, we implement the RFA routine with the function `rfa`. In the formula object below, the left-hand side variable is a binary response that equals 1 when an individual citizen turned out to vote in the 1998 congressional election in New Haven, CT. The treatment variable (whether the individual received a GOT phone call) is the right-hand side variable in the formula. The covariates to control for are given in the `covariates` command in a right-hand side only formula. These covariates are: an individual's age, whether they voted in the previous election (in 1996), the number of persons residing in their household, whether they are a new voter, whether they support the majority party, and their ward of residence in New Haven.

```{r}
rfa_fit <- rfa(
  formula = VOTED98 ~ PHN.C1, # treatment variable
  covariates = ~ AGE + VOTE96.1 + PERSONS + NEW + MAJORPTY + WARD, # confounders
  data = ggi
)
```

`rfa_fit` consists of a list containing the fitted regression results:

```{r}
rfa_fit$fit
```

the random forest regression for the response as a function of covariates:

```{r}
rfa_fit$yrf
```

the random forest regression for the causal variable of interest as a function of covariates:

```{r}
rfa_fit$xrf
```

and an updated dataframe that adds the processed response and explanatory variable to the data used in estimation:

```{r}
head(rfa_fit$data)
```

From the above, we see that the RFA routine estimates that the efect of a GOT phone call on the probability of turning out to vote is approximately `r round(tidy(rfa_fit$fit)[2,2],3)`. A summary for this estimate is provided by using `summary_rfa`. The output also includes the OOB (out of bag) $R^2$ for the random forest regressions for the response and the explanatory variable.

```{r}
summary_rfa(rfa_fit) %>%
  mutate_if(is.numeric, function(x) round(x, 3)) 
```

Sure enough, the ATE is statistically significant, with $p < 0.01$.

We can also use the `plot_rfa` function to plot the distribution of bootstrapped ATE replicates:

```{r}
plot_rfa(rfa_fit)
```

Further, since `plot_rfa` is a wrapper for `ggplot2`, we can modify the RFA plot however we see fit:

```{r}
plot_rfa(rfa_fit, varname = "Phone Call") +
  labs(
    x = "Estimated Effect of GOT",
    y = NULL,
    title = "I Like ggplot"
  ) +
  geom_vline(xintercept = 0, lty = 2) +
  theme_test()
```

