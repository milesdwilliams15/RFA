---
title: "Random Forest Adjustment with the `RFA` package"
author: "Miles D. Williams (University of Illinois at Urbana-Champaign, milesdw2@illinois.edu)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

### Details

The repository for this project can be accessed [here](https://github.com/milesdwilliams15/RFA). Check out [my website](https://milesdwilliams15.github.io/) to learn more about me and my research.

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(tidyverse)
results <- read_csv(paste0(getwd(),"/results.csv"))
```

## Introduction

Random Forest Adjustment (RFA) is a procedure for estimating the relationship between some predictor and a response after partialing out variation in each given a set of covariates via random forests. Below I provide a brief explanation of the technique and demonstrate how it is implemented in R.

## RFA in a Nutshell

RFA is a flexible approach to regression adjustment for controlling for covariates in observational settings. By leveraging a nonparametric machine learning algorithm such as random forests, RFA sidesteps the incidental functional form assumptions imposed by the standard multiple linear regression approach to covariate adjustment. This makes RFA robust to more complex (nonlinear or nonadditive) forms of confounding relationships in observational data that a parametric approach may fail to account for. 

RFA begins by predicting some response $y_i$ and some causal variable of interest $z_i$ as a function of covariates $x_{ip} \in X_i$ where $y_i,z_i \not\!\perp\!\!\!\perp X_i$. The variables $y_i$ and $z_i$ denote vectors of response and predictor values for the $i^\text{th}$ observation where $i \in I = \{1,2,...,n \}$. 

Because $z_i$ and $y_i$ are not independent of $X_i$, the estimated slope coefficient ($\alpha_1$) from the following linear model may not reflect the correct relationship between $z_i$ on $y_i$:
$$y_i = \alpha_0 + \alpha_1z_i + \epsilon_i. \tag{1}$$
RFA's solution to this problem is to partial out the variation in $y_i$ and $z_i$ explained by $X_i$ prior to estimating the relationship between the two using random forests to pre-process the explanatory variable and response. This is done via the following steps:

  1. Fit $y_i$ as a function of $X_i$ via random forests (the random forest fit for $y_i$ is denoted $\hat{f}_y(X_i)$), and then demean, or residualize, $y_i$ by its predicted conditional mean:

$$
\begin{aligned}
\hat{y}_i & = \hat{f}_y(X_i), \\
\hat{y}_i^\varepsilon & = y_i - \hat{y}_i.
\end{aligned} \tag{2}
$$

  2. Repeat this procedure for the explanatory variable of interest, $z_i$:

$$
\begin{aligned}
\hat{z}_i & = \hat{f}_z(X_i), \\
\hat{z}_i^\varepsilon & = z_i - \hat{z}_i.
\end{aligned} \tag{3}
$$

  3. Finally, the marginal relationship, adjusting for the confounding influence of $X_i$, is obtained by estimating the following linear model:

$$\hat{y}_i^\varepsilon = \beta_0 + \beta_1\hat{z}_i^\varepsilon + \mu_i.\tag{4}$$


# What is the Benefit of This Approach?

The advantage of RFA is that it sidesteps incidental functional form assumptions that the more conventional multiple regression adjustment strategy imposes. This can be easily demonstrated with a simple simulation. 

For said simulation:

  1. I set $n = 500$ and set the the parameter for the true relationship between a response and explanatory variable of interest ($\beta_1$) to 5.
  2. The data-generating process for the outcome variable $y_i$ is given as
  
$$y_i = 1 + 5z_i + 0.5x_i + x_i^2 + e_i:e_i \sim \mathcal{N}(0, \sigma = 10).$$

  3. The data generating process for the causal variable $z_i$ is given as
  
$$z_i = -1 + 0.05\space \text{stand}(x_i) - 0.1 \space \text{stand}(x_i)^2 + u_i: u_i \sim \mathcal{N}(0, \sigma = 2).$$

  4. The confounding variable $x_i \sim \mathcal{N}(50, \sigma = 10)$.

For each iteration of the simulated data-generating process, I recover estimates of the effect of $z_i$ on $y_i$ via:

  1. A naive linear regression model:
  
$$y_i = \beta_0 + \beta_1z_i + \epsilon_i.$$

  2. A a multiple regression model with controls:
  
$$y_i = \beta_0 + \beta_1z_i + \beta_2x_i + \epsilon_i.$$

  3. An interactive regression model with $x_i$ mean centered (otherwise known as the [Lin estimator](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-7/issue-1/Agnostic-notes-on-regression-adjustments-to-experimental-data--Reexamining/10.1214/12-AOAS583.full)):

$$y_i = \beta_0 + \beta_1z_i + \beta_2x_i + \beta_3z_i\cdot(x_i - \bar{x}) + \epsilon_i.$$

  4. Adjustment via a Support Vector Machine (SVM)---an alternative machine learner to random forests.
  
  5. Adjustment via random forests (RFA).
  
Because both SVM and random forests take a while to run, for time's sake I restricted the analysis to 100 iterations. 

A summary of the performance of each of these approaches is given in the below table. Metrics I use to gauge performance include root mean squared error (MSE), average bias, and coverage of the 95 percent confidence intervals. 

```{r, echo=FALSE}
results %>%
  group_by(model) %>%
  summarize(
    RMSE = sqrt(sum((estimate - ATE)^2)),
    "Ave. Bias" = mean(estimate - ATE),
    Coverage = mean((((estimate - 1.96*std.error)<=ATE) + 
                          ((estimate + 1.96*std.error)>=ATE))==2) 
  ) %>%
  mutate_if(is.numeric, function(x) round(x, 2)) %>%
  kableExtra::kable(format = "html")
```


RFA clearly performs the best out of all of the adjustment strategies. This is also clear from the following figure showing the distribution of estimates after 100 runs of the simulation. The estimates provided by RFA are the least prone to error and least biased. However, in terms of coverage, naive OLS, RFA, and SVA are relatively similar.

```{r, echo=FALSE, fig.width=6, fig.height=4}
ATE <- results$ATE[1]
results %>%
  select(-ATE) %>%
  dotwhisker::small_multiple(
    dot_args = list(alpha = 0.15)
  ) + 
  geom_hline(yintercept = c(0, ATE), linetype = c(1,2)) +
  labs(
    caption = "SVA = Support Vector Adjustment;\nRFA = Random Forest Adjustment\n(Dashed line denotes true effect)",
    y = "Estimate with 95% CI",
    title = "Comparison of Different Adjustment Strategies"
  ) +
  ggridges::theme_ridges(
    font_size = 10
  ) +
  theme(
    strip.background = element_blank(),
    strip.text = element_blank(),
    legend.position = "none"
  )
```


# How to Implement in R

The `rfa` function in the `RFA` package allows the researcher to easily implement the RFA routine. Here, I demonstrate how the program is used with the assistance of the `GerberGreenImai` dataset included in the `Matching` package. This dataset was used in Imai (2005) to replicate and extend Gerber and Green's (2000) get-out-the-vote (GOT) field experiment. The dataset was used to assess the causal effect of telephone calls on turnout. Note that this data is used for demonstration purposes only, and so results here should not be taken as confirmation or refutation of the findings of the original authors.

First, let's get the data into the working environment:

```{r, message=FALSE, warning=FALSE}
library(tidyverse) # grammar
library(Matching)  # for the data
data(GerberGreenImai)
ggi <- GerberGreenImai # give it a shorter name
```

Next, to implement RFA, we need to attach the `RFA` package:

```{r}
library(RFA)
```

If not already installed, the latest development version can be installed by entering `devtools::install_github("milesdwilliams15/RFA")` in the console.

The RFA routine is implemented with the function `rfa(...)`. The help file for the function can be accessed by entering `?rfa`. The function accepts the following arguments:


 - `formula`: a formula object where the left-hand variable is the outcome and the right-hand variable is the explanatory variable of interest.
  - `covariates`: a right-handed formula object specifying the covariates to be used in the random forest regressions.
  - `data`:	an optional data frame containing the variables used to implement the RFA routine.
  - `se_type`: specifies the standard errors to be returned. If 'clusters' is not specified, the user can specify "classical", "HC0", "stata" (equivalent to "HC1"), "HC2", or "HC3". If 'clusters' is specified, the options are "CR0", "stata" (CR1), and "CR2". "stata" is the default.
  - `clusters`: optional name (quoted) of variable that corresponds to clusters in the data.
  - `...`: additional commands to override the default settings for implementing random forests via 'ranger'. See the `ranger` package for more details.

Below is an example of how to use `rfa` with the `ggi` dataset. In the formula object below, the left-hand side variable is a binary response that equals 1 when an individual citizen turned out to vote in the 1998 congressional election in New Haven, CT. The treatment variable (whether the individual received a GOT phone call) is the right-hand side variable in the formula. The covariates to control for are given in the `covariates` command in a right-hand side only formula. These covariates are: an individual's age, whether they voted in the previous election (in 1996), the number of persons residing in their household, whether they are a new voter, whether they support the majority party, and their ward of residence in New Haven.

```{r}
rfa_fit <- rfa(
  formula = VOTED98 ~ PHN.C1, # treatment variable
  covariates = ~ AGE + VOTE96.1 + PERSONS + NEW + MAJORPTY + WARD, # confounders
  data = ggi
)
```

`rfa_fit` consists of a list containing the fitted regression results:

```{r}
rfa_fit$fit
```

the random forest regression for the response as a function of covariates:

```{r}
rfa_fit$yrf
```

the random forest regression for the causal variable of interest as a function of covariates:

```{r}
rfa_fit$xrf
```

and an updated dataframe that adds the processed response and explanatory variable to the data used in estimation:

```{r}
head(rfa_fit$data)
```

From the above, we see that the RFA routine estimates that the efect of a GOT phone call on the probability of turning out to vote is approximately `r round(tidy(rfa_fit$fit)[2,2],3)`. A summary for this estimate is provided by using `summary_rfa`. The output also includes the OOB (out of bag) $R^2$ for the random forest regressions for the response and the explanatory variable.

```{r}
summary_rfa(rfa_fit) %>%
  mutate_if(is.numeric, function(x) round(x, 3)) 
```

Sure enough, the ATE is statistically significant, with $p < 0.01$.

We can also use the `plot_rfa` function to plot the distribution of bootstrapped ATE replicates:

```{r}
plot_rfa(rfa_fit)
```

Further, since `plot_rfa` is a wrapper for `ggplot2`, we can modify the RFA plot however we see fit:

```{r}
plot_rfa(rfa_fit, varname = "Phone Call") +
  labs(
    x = "Estimated Effect of GOT",
    y = NULL,
    title = "I Like ggplot"
  ) +
  geom_vline(xintercept = 0, lty = 2) +
  theme_test()
```

